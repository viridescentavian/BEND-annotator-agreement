{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4uH6uGp6syR2TP4y7E/T7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Label Comparison\n",
        "\n",
        "**Task:** create an analytical program that compares BEND labeling between two different annotators.\n",
        "\n",
        "**Research Questions:**\n",
        "\n",
        "1) Are humans good detectors of the maneuvers?\n",
        "\n",
        "2) Are some maneuvers easier to detect by humans than others?\n",
        "\n",
        "3) How much overlap is there between labelers? Do they more or less match in what they are labeling?\n",
        "\n",
        "4) What is the correlation between the maneuvers and the agreement of labels between annotators? Calculate the agreement by maneuver between two coders. Maybe this goes back somewhat to question 2."
      ],
      "metadata": {
        "id": "oNsy7j_-x9tK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvo7kJ9nxfd1",
        "outputId": "5c0e73ec-01e7-49f4-802b-74775b0e6b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# setting up drive for data import\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Completed\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np\n",
        "\n",
        "# processing data for calculation, including loading and binarizing\n",
        "\n",
        "BEND_labels = ['Engage', 'Explain', 'Excite', 'Enhance', 'Dismiss', 'Distort', 'Dismay', 'Distract', 'Back', 'Build', 'Bridge', 'Boost', 'Neutralize', 'Nuke', 'Narrow', 'Neglect', 'NONE']\n",
        "completed_datasets = [\"CapnMarvel_1_aahmad1.csv\", \"CapnMarvel_1_yucongw.csv\", \"CapnMarvel_2_TPedireddi.csv\", \"CapnMarvel_2_yucongw.csv\", \"Election2020_20201102_4_aahmad1.csv\", \"Election2020_20201102_4_coco.csv\"]\n",
        "\n",
        "def load_data(data_path):\n",
        "    df = pd.read_csv(os.path.join(DATASET_PATH, data_path))\n",
        "    df[\"maneuver\"] = df[\"maneuver\"].fillna(\"NONE\")\n",
        "    return df[\"maneuver\"].tolist()\n",
        "\n",
        "def binarize(maneuver, data):\n",
        "  return [1 if maneuver in x else 0 for x in data]\n",
        "\n",
        "# implementing cohen's kappa\n",
        "\n",
        "def calculate_kappa():\n",
        "  kappas = []\n",
        "  for x in BEND_labels:\n",
        "      binarized1 = binarize(x, annotator1)\n",
        "      binarized2 = binarize(x, annotator2)\n",
        "      kappas.append(cohen_kappa_score(binarized1, binarized2))\n",
        "  return kappas\n",
        "\n",
        "def overall_agreement(scores):\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "# calculating annotator agreement through multiple datasets\n",
        "\n",
        "arr_scores = np.zeros(17)\n",
        "\n",
        "for i in range(0, 6, 2):\n",
        "  annotator1 = load_data(completed_datasets[i])\n",
        "  annotator2 = load_data(completed_datasets[i+1])\n",
        "  arr_scores = np.add(np.array(calculate_kappa()), arr_scores)\n",
        "\n",
        "arr_scores /= len(completed_datasets) / 2\n",
        "\n",
        "# printing results\n",
        "\n",
        "print(\"Annotator agreement by maneuver:\")\n",
        "for i in range(0, 17):\n",
        "  print('%-15s' '%s' % (BEND_labels[i], arr_scores[i]))\n",
        "print(f'Overall agreement between both annotators: {overall_agreement(arr_scores)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVygq0vNZuuG",
        "outputId": "a69afcf0-0259-473f-aebd-5d4b010525b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotator agreement by maneuver:\n",
            "Engage         0.1719889309739445\n",
            "Explain        0.2427149255131402\n",
            "Excite         0.36445016345346787\n",
            "Enhance        0.048521264862550484\n",
            "Dismiss        -0.0063405797101449375\n",
            "Distort        0.10213817841038982\n",
            "Dismay         0.3263528981968355\n",
            "Distract       0.08281523991710504\n",
            "Back           0.04380936799871941\n",
            "Build          0.11491840719474185\n",
            "Bridge         -0.021021821907249622\n",
            "Boost          0.07654837656807412\n",
            "Neutralize     0.1850906974837134\n",
            "Nuke           0.27209916683600893\n",
            "Narrow         0.1621152872485969\n",
            "Neglect        -0.029075893616940036\n",
            "NONE           0.2262449038073281\n",
            "Overall agreement between both annotators: 0.1390217360723695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating frequencies of maneuvers in the datasets\n",
        "\n",
        "total_frequencies = np.zeros(17)\n",
        "\n",
        "def frequency(annotations):\n",
        "  maneuver_frequencies = []\n",
        "  for x in BEND_labels:\n",
        "    binarized = binarize(x, annotations)\n",
        "    maneuver_frequencies.append(sum(binarized))\n",
        "  return maneuver_frequencies\n",
        "\n",
        "for i in range(0, 6):\n",
        "  total_frequencies = np.add(total_frequencies, frequency(load_data(completed_datasets[i])))\n",
        "\n",
        "print(\"Frequency of maneuvers:\")\n",
        "for i in range(0, 17):\n",
        "  print('%-15s' '%s' % (BEND_labels[i], total_frequencies[i]))\n",
        "\n",
        "percentages = np.divide(total_frequencies, sum(total_frequencies))\n",
        "percentages = np.multiply(percentages, 1000)\n",
        "\n",
        "print(\"Percent frequency of maneuvers:\")\n",
        "for i in range(0, 17):\n",
        "  print('%-15s' '%s' % (BEND_labels[i], percentages[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj5o4bkTvG0b",
        "outputId": "5544ec6b-96c1-4482-9c8a-2f8049de5ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of maneuvers:\n",
            "Engage         75.0\n",
            "Explain        167.0\n",
            "Excite         159.0\n",
            "Enhance        85.0\n",
            "Dismiss        58.0\n",
            "Distort        88.0\n",
            "Dismay         206.0\n",
            "Distract       13.0\n",
            "Back           153.0\n",
            "Build          96.0\n",
            "Bridge         19.0\n",
            "Boost          92.0\n",
            "Neutralize     127.0\n",
            "Nuke           39.0\n",
            "Narrow         42.0\n",
            "Neglect        53.0\n",
            "NONE           59.0\n",
            "Percent frequency of maneuvers:\n",
            "Engage         48.98758981058132\n",
            "Explain        109.07903331156108\n",
            "Excite         103.85369039843239\n",
            "Enhance        55.51926845199216\n",
            "Dismiss        37.88373612018289\n",
            "Distort        57.478772044415415\n",
            "Dismay         134.55258001306336\n",
            "Distract       8.491182233834095\n",
            "Back           99.9346832135859\n",
            "Build          62.70411495754409\n",
            "Bridge         12.4101894186806\n",
            "Boost          60.09144350097975\n",
            "Neutralize     82.9523187459177\n",
            "Nuke           25.473546701502286\n",
            "Narrow         27.433050293925536\n",
            "Neglect        34.617896799477464\n",
            "NONE           38.53690398432397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting Cohen's kappa\n",
        "\n",
        "`<0`: no agreement\n",
        "\n",
        "`0-0.20`: slight agreement\n",
        "\n",
        "`0.21-0.40`: fair agreement\n",
        "\n",
        "`0.41-0.60`: moderate agreement\n",
        "\n",
        "`0.61-0.80`: substantial agreement\n",
        "\n",
        "`0.81-1`: perfect agreement"
      ],
      "metadata": {
        "id": "0QI8Vju2-tq-"
      }
    }
  ]
}